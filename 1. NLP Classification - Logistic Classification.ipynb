{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement Logistic Classification for classifying tweets / text\n",
    "Given a tweet we will have to decide whether a tweet is positive and negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import twitter_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Analyse the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load positive tweets\n",
    "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "positive_tweets[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load negative tweets\n",
    "negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "negative_tweets[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## total number of pos and neg tweets\n",
    "\n",
    "print(f\"Total No. of Positive tweets: {len(positive_tweets)}\")\n",
    "print(f'Total No. of Negative tweets: {len(negative_tweets)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'positive_tweets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/edz/Documents/Course/NLP/notebooks/MLPy/nlp/1. NLP Classification - Logistic Classification.ipynb Cell 9\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/edz/Documents/Course/NLP/notebooks/MLPy/nlp/1.%20NLP%20Classification%20-%20Logistic%20Classification.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m## generate a train and test dataset with equal combination of pos and neg tweets\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/edz/Documents/Course/NLP/notebooks/MLPy/nlp/1.%20NLP%20Classification%20-%20Logistic%20Classification.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m## in total 1000 words, dividing the list of tweets into 8000 train and 2000 test datasets.\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/edz/Documents/Course/NLP/notebooks/MLPy/nlp/1.%20NLP%20Classification%20-%20Logistic%20Classification.ipynb#X11sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m train_pos \u001b[39m=\u001b[39m positive_tweets[:\u001b[39m4000\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/edz/Documents/Course/NLP/notebooks/MLPy/nlp/1.%20NLP%20Classification%20-%20Logistic%20Classification.ipynb#X11sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m train_neg \u001b[39m=\u001b[39m negative_tweets[:\u001b[39m4000\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/edz/Documents/Course/NLP/notebooks/MLPy/nlp/1.%20NLP%20Classification%20-%20Logistic%20Classification.ipynb#X11sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m test_pos \u001b[39m=\u001b[39m positive_tweets[\u001b[39m4000\u001b[39m:]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'positive_tweets' is not defined"
     ]
    }
   ],
   "source": [
    "## generate a train and test dataset with equal combination of pos and neg tweets\n",
    "## in total 1000 words, dividing the list of tweets into 8000 train and 2000 test datasets.\n",
    "\n",
    "train_pos = positive_tweets[:4000]\n",
    "train_neg = negative_tweets[:4000]\n",
    "\n",
    "test_pos = positive_tweets[4000:]\n",
    "test_neg = negative_tweets[4000:]\n",
    "\n",
    "# combining all of them together\n",
    "\n",
    "train_data = train_pos + train_neg\n",
    "test_data = test_pos + test_neg\n",
    "\n",
    "print(f'Total number of data count train data: {len(train_data)} and test data : {len(test_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating labels for the datasets\n",
    "train_label = np.append(np.ones((len(train_pos),1)), np.zeros((len(train_neg),1)), axis=0)\n",
    "test_label = np.append(np.ones((len(test_pos),1)), np.zeros((len(test_neg),1)), axis=0)\n",
    "\n",
    "print(f'Shape of Train and Test labels : {train_label.shape} and {test_label.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing of the data to create word frequencies list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "def clean_tweet(tweet):\n",
    "    '''\n",
    "        clean the tweet to tokenise, remove stop words and stem the words\n",
    "    '''\n",
    "    stop_words = stopwords.words('english')\n",
    "    #print(f'Total stop words in the vocab: {len(stop_words)}')\n",
    "    \n",
    "    tweet = re.sub(r'#','',tweet) ## remove the # symbol\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*','',tweet) ## remove any hyperlinks\n",
    "    tweet = re.sub(r'^RT[\\s]+','',tweet) ## remove any Retweets (RT)\n",
    "    \n",
    "    tokenizer = nltk.tokenize.TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "    tweet_token = tokenizer.tokenize(tweet)\n",
    "    \n",
    "    tweet_cleaned = []\n",
    "    \n",
    "    for word in tweet_token:\n",
    "        if word not in stop_words:\n",
    "            tweet_cleaned.append(word)\n",
    "            \n",
    "    return tweet_cleaned\n",
    "    \n",
    "\n",
    "def build_tweet_frequency(tweets, label):\n",
    "    '''\n",
    "        Build a vocab of tweet word frequencies across corpus. \n",
    "        @input: Tweets - list of tweets\n",
    "                label - Array of tweet sentiments\n",
    "        @output: a dict of (word, label):frequency\n",
    "    '''\n",
    "    label_list = np.squeeze(label).tolist()\n",
    "    \n",
    "    freq = {}\n",
    "    \n",
    "    for t, l in zip(tweets, label_list):\n",
    "        for word in clean_tweet(t):\n",
    "            word_pair = (word,l)\n",
    "            \n",
    "            if word_pair in freq:\n",
    "                freq[word_pair] +=1\n",
    "            else:\n",
    "                freq[word_pair] =1\n",
    "\n",
    "    return freq\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[0] ## 0, 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_tweet(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_freq_vocab = build_tweet_frequency(train_data, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_freq_vocab.get(('sad',0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(tweet, vocab):\n",
    "    '''\n",
    "        Given a tweet and frequency vocab, generate a list of \n",
    "        @input: \n",
    "            tweet - tweet we want to extract features from\n",
    "            vocab - frequency vocab dictionary\n",
    "        @output:\n",
    "            tweet_feature - a numpy array with [label, total_pos_freq, total_neg_freq]\n",
    "    '''\n",
    "    cleaned_tweet = clean_tweet(tweet)\n",
    "    #print(cleaned_tweet)\n",
    "    tweet_feature = np.zeros((1,3))\n",
    "    \n",
    "    tweet_feature[0,0] = 1\n",
    "    \n",
    "    for words in cleaned_tweet: # iterate over the tweet to get the number of pos and neg tweet freqs\n",
    "        #print(vocab.get((words,1.0),0), \" --- \", vocab.get((words,0.0),0))\n",
    "        tweet_feature[0,1] += vocab.get((words,1.0),0)\n",
    "        tweet_feature[0,2] += vocab.get((words,0.0),0)\n",
    "    \n",
    "    return tweet_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_features(train_data[0],tweet_freq_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_features('Hi How are you? I am doing good', tweet_freq_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate the vector word frequency for all of the training tweets\n",
    "\n",
    "train_X = np.zeros((len(train_data),3))\n",
    "for i in range(len(train_data)):\n",
    "    train_X[i,:] = extract_features(train_data[i], tweet_freq_vocab)\n",
    "\n",
    "train_y = train_label\n",
    "\n",
    "test_X = np.zeros((len(test_data),3))\n",
    "for i in range(len(test_data)):\n",
    "    test_X[i,:] = extract_features(test_data[i], tweet_freq_vocab)\n",
    "    \n",
    "test_y = test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(solver='liblinear')\n",
    "model.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(test_y, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(test_y,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making your own predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tweet1 = 'i liked my prediction score. happy with the results'\n",
    "model.predict(extract_features(my_tweet1,tweet_freq_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tweet2 = 'i am sad with the result of the football match'\n",
    "model.predict(extract_features(my_tweet2,tweet_freq_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tweet3 = 'shame that i couldnt get an entry to the competition'\n",
    "model.predict(extract_features(my_tweet3,tweet_freq_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tweet3 = 'this movie should have been great.'\n",
    "model.predict(extract_features(my_tweet3,tweet_freq_vocab)) ## misclassified example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tweet3 = 'i liked my prediction score. not happy with the results'\n",
    "model.predict(extract_features(my_tweet3,tweet_freq_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tweet4 = 'My boss is a true genius'\n",
    "model.predict(extract_features(my_tweet4,tweet_freq_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tweet5 = 'I have the greatest boss in the world'\n",
    "model.predict(extract_features(my_tweet5,tweet_freq_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "course",
   "language": "python",
   "name": "course"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
