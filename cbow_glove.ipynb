{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/edzimmermann/NLP/blob/main/cbow_glove.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2tFG9OXa0auj"
      },
      "outputs": [],
      "source": [
        "# import pytorch libraries\n",
        "%matplotlib inline\n",
        "import torch\n",
        "import torch.autograd as autograd\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJk3Xb4M0aum"
      },
      "source": [
        "# Text Classification\n",
        "In this part of the tutorial we develop a continuous bag of words (CBOW) model for a text classification task described [here]( https://people.cs.umass.edu/~miyyer/pubs/2015_acl_dan.pdf). The CBOW model was first described [here](https://arxiv.org/pdf/1301.3781.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7FBEJQ20aun"
      },
      "source": [
        "## Subjectivity Dataset\n",
        "The subjectivity dataset has 5000 subjective and 5000 objective processed sentences. To get the data:\n",
        "```\n",
        "wget http://www.cs.cornell.edu/people/pabo/movie-review-data/rotten_imdb.tar.gz\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oB22DNbT0aun"
      },
      "outputs": [],
      "source": [
        "def unpack_dataset():\n",
        "    ! wget http://www.cs.cornell.edu/people/pabo/movie-review-data/rotten_imdb.tar.gz\n",
        "    ! mkdir data\n",
        "    ! tar -xvf rotten_imdb.tar.gz -C data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jHgEZ17q0aun"
      },
      "outputs": [],
      "source": [
        "#unpack_dataset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "InTKlr3A0auo",
        "outputId": "a0c65ab7-a428-4500-acc9-0d92aa8a064a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ls: cannot access 'data': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!ls data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_2ve9uJI0auo",
        "outputId": "c868a888-b1ba-4436-dd23-233f173471e7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "head: cannot open 'data/plot.tok.gt9.5000' for reading: No such file or directory\n"
          ]
        }
      ],
      "source": [
        "! head -2 data/plot.tok.gt9.5000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dSHi-fwR0aup",
        "outputId": "b790e461-de89-48a8-c7af-9209be9feed2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'data'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-0f58b9fe54d5>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpathlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mPATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/lib/python3.10/pathlib.py\u001b[0m in \u001b[0;36miterdir\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1015\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mspecial\u001b[0m \u001b[0mpaths\u001b[0m \u001b[0;34m'.'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m'..'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m         \"\"\"\n\u001b[0;32m-> 1017\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1018\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'..'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m                 \u001b[0;31m# Yielding a path object for these makes little sense\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data'"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "PATH = Path(\"data\")\n",
        "list(PATH.iterdir())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fe7xHwqd0aup"
      },
      "source": [
        "## Tokenization\n",
        "Tokenization is the task of chopping up text into pieces, called tokens.\n",
        "\n",
        "spaCy is an open-source software library for advanced Natural Language Processing. Here we will use it for tokenization.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpYbfaVF0aup"
      },
      "source": [
        "### Simple Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hed6A8Vy0auq"
      },
      "outputs": [],
      "source": [
        "# We need each line in the file\n",
        "def read_file(path):\n",
        "    \"\"\" Read file returns a list of lines.\n",
        "    \"\"\"\n",
        "    with open(path, encoding = \"ISO-8859-1\") as f:\n",
        "        content = f.readlines()\n",
        "    return content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WpzMLhli0auq"
      },
      "outputs": [],
      "source": [
        "obj_lines = read_file(PATH/\"plot.tok.gt9.5000\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PAYMmPDB0auq"
      },
      "outputs": [],
      "source": [
        "obj_lines[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VUklFxdG0aur"
      },
      "outputs": [],
      "source": [
        "np.array(obj_lines[0].strip().lower().split(\" \"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HUpgUm10aur"
      },
      "source": [
        "### Much better tokenization with Spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "STV6hMah0aur"
      },
      "outputs": [],
      "source": [
        "#!pip install -U spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uiMqLF7x0aus"
      },
      "outputs": [],
      "source": [
        "import spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6NRceZLP0aus"
      },
      "outputs": [],
      "source": [
        "# first time run this\n",
        "#!python3 -m spacy download en"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NOQ_fksP0aus"
      },
      "outputs": [],
      "source": [
        "tok = spacy.load('en')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6KF6lp1r0aus"
      },
      "outputs": [],
      "source": [
        "obj_lines = read_file(PATH/\"plot.tok.gt9.5000\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wKdwiCfU0aus"
      },
      "outputs": [],
      "source": [
        "len(obj_lines)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kwky227t0aut"
      },
      "outputs": [],
      "source": [
        "obj_lines[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0AsbQYLG0aut"
      },
      "outputs": [],
      "source": [
        "test = tok(obj_lines[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jUm3a_Vp0aut"
      },
      "outputs": [],
      "source": [
        "np.array([x for x in test])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cuFU_2J0aut"
      },
      "source": [
        "## Split dataset in train and validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CxnCdIXf0aut"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zYIhI-jV0aut"
      },
      "outputs": [],
      "source": [
        "sub_content = read_file(PATH/\"quote.tok.gt9.5000\")\n",
        "obj_content = read_file(PATH/\"plot.tok.gt9.5000\")\n",
        "sub_content = np.array([line.strip().lower() for line in sub_content])\n",
        "obj_content = np.array([line.strip().lower() for line in obj_content])\n",
        "sub_y = np.zeros(len(sub_content))\n",
        "obj_y = np.ones(len(obj_content))\n",
        "X = np.append(sub_content, obj_content)\n",
        "y = np.append(sub_y, obj_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2dE0RYWG0auu"
      },
      "outputs": [],
      "source": [
        "X[0], y[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aJt7cKIG0auu"
      },
      "outputs": [],
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WDJzxF3l0auu"
      },
      "outputs": [],
      "source": [
        "X_train[:5], y_train[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4as6F8cW0auu"
      },
      "source": [
        "## Word to index mapping\n",
        "In interest of time we will tokenize without spaCy. Here we will compute a vocabulary of words based on the training set and a mapping from word to an index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZjP9IdBe0auu"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gccawxss0auv"
      },
      "outputs": [],
      "source": [
        "def get_vocab(content):\n",
        "    \"\"\"Computes Dict of counts of words.\n",
        "\n",
        "    Computes the number of times a word is on a document.\n",
        "    \"\"\"\n",
        "    vocab = defaultdict(float)\n",
        "    for line in content:\n",
        "        words = set(line.split())\n",
        "        for word in words:\n",
        "            vocab[word] += 1\n",
        "    return vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pG3Jsk5X0auv"
      },
      "outputs": [],
      "source": [
        "#Getting the vocabulary from the training set\n",
        "word_count = get_vocab(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QIyKIAAo0auv"
      },
      "outputs": [],
      "source": [
        "#word_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MCV40EhY0auw"
      },
      "outputs": [],
      "source": [
        "len(word_count.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_hAfoZID0auw"
      },
      "outputs": [],
      "source": [
        "# let's delete words that are very infrequent\n",
        "for word in list(word_count):\n",
        "    if word_count[word] < 5:\n",
        "        del word_count[word]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-IpSseuW0auw"
      },
      "outputs": [],
      "source": [
        "len(word_count.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQsieE9y0aux"
      },
      "outputs": [],
      "source": [
        "## Finally we need an index for each word in the vocab\n",
        "vocab2index = {\"<PAD>\":0, \"UNK\":1} # init with padding and unknown\n",
        "words = [\"<PAD>\", \"UNK\"]\n",
        "for word in word_count:\n",
        "    vocab2index[word] = len(words)\n",
        "    words.append(word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YCTVSnD70aux"
      },
      "outputs": [],
      "source": [
        "#vocab2index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JeRezKJn0auy"
      },
      "source": [
        "## Sentence encoding\n",
        "Here we encode each sentence as a sequence of indices corresponding to each word."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N5wkT9eP0auy"
      },
      "outputs": [],
      "source": [
        "x_train_len = np.array([len(x.split()) for x in X_train])\n",
        "x_val_len = np.array([len(x.split()) for x in X_val])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CJEBMRFc0auz"
      },
      "outputs": [],
      "source": [
        "np.percentile(x_train_len, 95) # let set the max sequence len to N=40"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "opJa2dC40auz"
      },
      "outputs": [],
      "source": [
        "X_train[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sdf9BvD80au0"
      },
      "outputs": [],
      "source": [
        "# returns the index of the word or the index of \"UNK\" otherwise\n",
        "vocab2index.get(\"?\", vocab2index[\"UNK\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8U_pupLV0au0"
      },
      "outputs": [],
      "source": [
        "np.array([vocab2index.get(w, vocab2index[\"UNK\"]) for w in X_train[0].split()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DulGZqw70au0"
      },
      "outputs": [],
      "source": [
        "def encode_sentence(s, N=40):\n",
        "    enc = np.zeros(N, dtype=np.int32)\n",
        "    enc1 = np.array([vocab2index.get(w, vocab2index[\"UNK\"]) for w in s.split()])\n",
        "    l = min(N, len(enc1))\n",
        "    enc[:l] = enc1[:l]\n",
        "    return enc, l"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OUM-BqX30au0"
      },
      "outputs": [],
      "source": [
        "encode_sentence(X_train[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ow2ugMn0au1"
      },
      "outputs": [],
      "source": [
        "x_train_len = np.minimum(x_train_len, 40)\n",
        "x_val_len = np.minimum(x_val_len, 40)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "boC-Rhyq0au1"
      },
      "outputs": [],
      "source": [
        "x_train = np.vstack([encode_sentence(x) for x in X_train])\n",
        "x_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-aoHR4S10au1"
      },
      "outputs": [],
      "source": [
        "x_val = np.vstack([encode_sentence(x) for x in X_val])\n",
        "x_val.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qqYNaIz0au1"
      },
      "source": [
        "## Embedding layer\n",
        "Most deep learning models use a dense vectors of real numbers as representation of words (word embeddings), as opposed to a one-hot encoding representations. The module torch.nn.Embedding is used to represent word embeddings. It takes two arguments: the vocabulary size, and the dimensionality of the embeddings. The embeddings are initialized with random vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1RemUmbn0au1"
      },
      "outputs": [],
      "source": [
        "# an Embedding module containing 10 words with embedding size 4\n",
        "# embedding will be initialized at random\n",
        "embed = nn.Embedding(10, 4, padding_idx=0)\n",
        "embed.weight"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YkLVrHu0au1"
      },
      "source": [
        "Note that the `padding_idx` has embedding vector 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WkBFMM6D0au1"
      },
      "outputs": [],
      "source": [
        "# given a list of ids we can \"look up\" the embedding corresponing to each id\n",
        "# can you see that some vectors are the same?\n",
        "a = torch.LongTensor([[1,4,1,5,1,0]])\n",
        "embed(a)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtScGIuB0au2"
      },
      "source": [
        "This would be the representation of a sentence with words with indices [1,4,1,5,1] and a padding at the end. Bellow we have an example in which we have two sentences. the first sentence has length 3 and the last sentence has length 2. In order to use a tensor we use padding at the end of the second sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zPQa7xsT0au2"
      },
      "outputs": [],
      "source": [
        "a = torch.LongTensor([[1,4,1], [1,3,0]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyofNtwk0au2"
      },
      "source": [
        "Our model takes an average of the word embedding of each word. Here is how we do it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IGD6vFSV0au2"
      },
      "outputs": [],
      "source": [
        "s = torch.FloatTensor([3, 2]) # here is the size of the vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mmHuRCpT0au2"
      },
      "outputs": [],
      "source": [
        "embed(a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yF5uHjvH0au2"
      },
      "outputs": [],
      "source": [
        "embed(a).sum(dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TPMNNj9h0au2"
      },
      "outputs": [],
      "source": [
        "sum_embs = embed(a).sum(dim=1)\n",
        "sum_embs/ s.view(s.shape[0], 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtN3IGdQ0au2"
      },
      "source": [
        "## Continuous Bag of Words Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hpg66unf0au2"
      },
      "outputs": [],
      "source": [
        "class CBOW(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_size=100):\n",
        "        super(CBOW, self).__init__()\n",
        "        self.word_emb = nn.Embedding(vocab_size, emb_size, padding_idx=0)\n",
        "        self.linear = nn.Linear(emb_size, 1)\n",
        "\n",
        "    def forward(self, x, s):\n",
        "        x = self.word_emb(x)\n",
        "        x = x.sum(dim=1)/ s\n",
        "        x = self.linear(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtCU60pV0au2"
      },
      "source": [
        "# Training the CBOW model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j5wJttRn0au2"
      },
      "outputs": [],
      "source": [
        "V = len(words)\n",
        "model = CBOW(vocab_size=V, emb_size=50)\n",
        "print(V)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pGAUSoKU0au3"
      },
      "outputs": [],
      "source": [
        "class SubjectivityDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.x = X\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.x[idx]\n",
        "        x, s = encode_sentence(x)\n",
        "        return x, self.y[idx], s\n",
        "\n",
        "train_ds = SubjectivityDataset(X_train, y_train)\n",
        "valid_ds = SubjectivityDataset(X_val, y_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nuTdzWcP0au3"
      },
      "outputs": [],
      "source": [
        "train_dl = DataLoader(train_ds, batch_size=5, shuffle=True)\n",
        "x, y, s = next(iter(train_dl))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aoqsR1nQ0au3"
      },
      "outputs": [],
      "source": [
        "x, y, s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wuc7xlC-0au3"
      },
      "outputs": [],
      "source": [
        "model = CBOW(vocab_size=V, emb_size=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-5C59Iay0au3"
      },
      "outputs": [],
      "source": [
        "train_dl = DataLoader(train_ds, batch_size=500, shuffle=True)\n",
        "valid_dl = DataLoader(valid_ds, batch_size=500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dRy4PtCr0au3"
      },
      "outputs": [],
      "source": [
        "def valid_metrics(model):\n",
        "    model.eval()\n",
        "    total = 0\n",
        "    sum_loss = 0\n",
        "    correct = 0\n",
        "    for x, y, s in valid_dl:\n",
        "        x = x.long()  #.cuda()\n",
        "        y = y.float().unsqueeze(1)\n",
        "        s = s.float().view(s.shape[0], 1)\n",
        "        batch = y.shape[0]\n",
        "        out = model(x, s)\n",
        "        loss = F.binary_cross_entropy_with_logits(out, y)\n",
        "        sum_loss += batch*(loss.item())\n",
        "        total += batch\n",
        "        pred = (out > 0).float()\n",
        "        correct += (pred == y).float().sum().item()\n",
        "    val_loss = sum_loss/total\n",
        "    val_acc = correct/total\n",
        "    return val_loss, val_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gbGekE-v0au3"
      },
      "outputs": [],
      "source": [
        "def train_epocs(model, optimizer, epochs=10):\n",
        "    for i in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        total = 0\n",
        "        for x, y, s in train_dl:\n",
        "            x = x.long()\n",
        "            y = y.float().unsqueeze(1)\n",
        "            s = s.float().view(s.shape[0], 1)\n",
        "            out = model(x, s)\n",
        "            loss = F.binary_cross_entropy_with_logits(out, y)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += x.size(0)*loss.item()\n",
        "            total += x.size(0)\n",
        "        train_loss = total_loss/total\n",
        "        val_loss, val_accuracy = valid_metrics(model)\n",
        "\n",
        "        print(\"train_loss %.3f val_loss %.3f val_accuracy %.3f\" % (\n",
        "            train_loss, val_loss, val_accuracy))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d58WcMJL0au3"
      },
      "outputs": [],
      "source": [
        "model = CBOW(vocab_size=V, emb_size=50)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "train_epocs(model, optimizer, epochs=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWXS9lXs0au4"
      },
      "source": [
        "## Using pre-trained embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5GgVixx0au4"
      },
      "source": [
        "To get glove pre-trained embeddings:\n",
        "    `wget http://nlp.stanford.edu/data/glove.6B.zip` or use this function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-sOEk7rd0au4"
      },
      "outputs": [],
      "source": [
        "def unpack_glove():\n",
        "    ! wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "    ! mkdir data\n",
        "    ! unzip glove.6B.zip -C data\n",
        "\n",
        "# unpack_glove()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QxkHXTAn0au4"
      },
      "outputs": [],
      "source": [
        "! head -2 data/glove.6B.50d.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWYOoDKI0au4"
      },
      "source": [
        "We would like to initialize the embeddings from our model with the pre-trained Glove embeddings. After initializing we should \"freeze\" the embeddings at least initially. The rationale is that we first want the network to learn weights for the other parameters that were randomly initialize. After that phase we could finetune the embeddings to our task.\n",
        "\n",
        "`embed.weight.requires_grad = False` freezes the embedding parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJOq7l960au4"
      },
      "source": [
        "The following code initializes the embedding. Here `V` is the vocabulary size and `emb_size` is the embedding size. `pretrained_weight` is a numpy matrix of shape `(V, emb_size)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "te0qRZsE0au4"
      },
      "outputs": [],
      "source": [
        "def loadGloveModel(gloveFile=PATH/\"glove.6B.50d.txt\"):\n",
        "    \"\"\" Loads word vectors into a dictionary.\"\"\"\n",
        "    f = open(gloveFile,'r')\n",
        "    word_vecs = {}\n",
        "    for line in f:\n",
        "        splitLine = line.split()\n",
        "        word = splitLine[0]\n",
        "        word_vecs[word] = np.array([float(val) for val in splitLine[1:]])\n",
        "    return word_vecs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-j_2FZvC0au4"
      },
      "outputs": [],
      "source": [
        "word_vecs = loadGloveModel()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zskT1_yA0au4"
      },
      "outputs": [],
      "source": [
        "# let compute the vocab again\n",
        "word_count = get_vocab(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y8T5pcJE0au4"
      },
      "outputs": [],
      "source": [
        "print(len(word_vecs.keys()), len(word_count.keys()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xft5jTkY0au5"
      },
      "outputs": [],
      "source": [
        "def delete_rare_words(word_vecs, word_count, min_df=4):\n",
        "    \"\"\" Deletes rare words from word_count\n",
        "\n",
        "    Deletes words from word_count if they are not in word_vecs\n",
        "    and don't have at least min_df occurrencies in word_count.\n",
        "    \"\"\"\n",
        "    words_delete = []\n",
        "    for word in word_count:\n",
        "        if word_count[word] < min_df and word not in word_vecs:\n",
        "            words_delete.append(word)\n",
        "    for word in words_delete: word_count.pop(word)\n",
        "    return word_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DscEtgtr0au5"
      },
      "outputs": [],
      "source": [
        "word_count = delete_rare_words(word_vecs, word_count)\n",
        "print(len(word_count.keys()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P1ukwMUX0au5"
      },
      "outputs": [],
      "source": [
        "def create_embedding_matrix(word_vecs, word_count, min_df=4, emb_size=50):\n",
        "    \"\"\"Creates embedding matrix from word vectors. \"\"\"\n",
        "    word_count = delete_rare_words(word_vecs, word_count, min_df)\n",
        "    V = len(word_count.keys()) + 2\n",
        "    vocab2index = {}\n",
        "    W = np.zeros((V, emb_size), dtype=\"float32\")\n",
        "    vocab = [\"\", \"UNK\"]\n",
        "    # adding a vector for padding\n",
        "    W[0] = np.zeros(emb_size, dtype='float32')\n",
        "    # adding a vector for rare words\n",
        "    W[1] = np.random.uniform(-0.25, 0.25, emb_size)\n",
        "    vocab2index[\"UNK\"] = 1\n",
        "    i = 2\n",
        "    for word in word_count:\n",
        "        if word in word_vecs:\n",
        "            W[i] = word_vecs[word]\n",
        "            vocab2index[word] = i\n",
        "            vocab.append(word)\n",
        "            i += 1\n",
        "        else:\n",
        "            W[i] = np.random.uniform(-0.25,0.25, emb_size)\n",
        "            vocab2index[word] = i\n",
        "            vocab.append(word)\n",
        "            i += 1\n",
        "    return W, np.array(vocab), vocab2index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JZT_x02J0au5"
      },
      "outputs": [],
      "source": [
        "pretrained_weight, vocab, vocab2index = create_embedding_matrix(word_vecs, word_count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CUcfv41t0au5"
      },
      "outputs": [],
      "source": [
        "len(pretrained_weight)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RpqYEl-Z0au5"
      },
      "outputs": [],
      "source": [
        "# creating an embedding matrix with Glove embeddings\n",
        "emb_size = 50\n",
        "V = len(pretrained_weight)\n",
        "emb = nn.Embedding(V, emb_size)\n",
        "emb.weight.data.copy_(torch.from_numpy(pretrained_weight))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbA5KQeR0au5"
      },
      "source": [
        "## Model with pre-trained embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hT9ToSi70au5"
      },
      "outputs": [],
      "source": [
        "class CBOW(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_size=50, glove_weights=None):\n",
        "        super(CBOW, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_size, padding_idx=0)\n",
        "        if glove_weights is not None:\n",
        "            self.embedding.weight.data.copy_(torch.from_numpy(glove_weights))\n",
        "            self.embedding.weight.requires_grad = False ## freeze embeddings\n",
        "\n",
        "        self.linear = nn.Linear(emb_size, 1)\n",
        "\n",
        "    def forward(self, x, s):\n",
        "        x = self.embedding(x)\n",
        "        x = x.sum(dim=1)/ s\n",
        "        x = self.linear(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vLTW7JQc0au5"
      },
      "outputs": [],
      "source": [
        "def set_learning_rate(optimizer, lr):\n",
        "    \"\"\"Changing learning rates without creating a new optimizer\"\"\"\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3dH8g-X60au5"
      },
      "outputs": [],
      "source": [
        "model = CBOW(V, emb_size=50, glove_weights=pretrained_weight)\n",
        "parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
        "optimizer = torch.optim.Adam(parameters, lr=0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dN2lahNf0au6"
      },
      "outputs": [],
      "source": [
        "train_epocs(model, optimizer, epochs=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Aw_37JN0au6"
      },
      "outputs": [],
      "source": [
        "# unfreezing the embeddings\n",
        "model.embedding.weight.requires_grad = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IYXuZzgd0au6"
      },
      "outputs": [],
      "source": [
        "parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
        "optimizer = torch.optim.Adam(parameters, lr=0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3IaRRSCt0au6"
      },
      "outputs": [],
      "source": [
        "train_epocs(model, optimizer, epochs=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-snpYk7Z0au6"
      },
      "outputs": [],
      "source": [
        "set_learning_rate(optimizer, 0.001)\n",
        "train_epocs(model, optimizer, epochs=10)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "nav_menu": {},
    "toc": {
      "nav_menu": {
        "height": "116px",
        "width": "251px"
      },
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "toc_cell": true,
      "toc_position": {},
      "toc_section_display": "block",
      "toc_window_display": false
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}